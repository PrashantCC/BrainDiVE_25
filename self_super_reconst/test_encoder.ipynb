{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca29520c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'self_super_reconst'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2da71a0f4482>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpretrainedmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpretrainedmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpmutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mself_super_reconst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfig_enc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabsl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'self_super_reconst'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Train image to fMRI encoder by supervised learning.\n",
    "    Date created: 8/25/19\n",
    "    Python Version: 3.6\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Guy Gaziv\"\n",
    "__email__ = \"guy.gaziv@weizmann.ac.il\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import pretrainedmodels as pm\n",
    "import pretrainedmodels.utils as pmutils\n",
    "from self_super_reconst.utils import *\n",
    "from config_enc import *\n",
    "from absl import app\n",
    "from utils.misc import set_gpu\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "from load_algo import get_NSD_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b04a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "    import pdb; pdb.set_trace()\n",
    "    del argv\n",
    "    cprint1(FLAGS.exp_prefix)\n",
    "    cprint1('== Summary level: {} =='.format(FLAGS.sum_level))\n",
    "\n",
    "    set_gpu()                     # Commented Achin 2704\n",
    "\n",
    "    # Data\n",
    "    # fmri_transform = np.float32\n",
    "    # get_dataset = lambda subset_case: \\\n",
    "    #     KamitaniDataset(FLAGS.roi, sbj_num=FLAGS.sbj_num, im_res=FLAGS.im_res, fmri_xfm=fmri_transform, subset_case=subset_case, select_voxels=FLAGS.select_voxels, is_rgbd=FLAGS.is_rgbd)\n",
    "\n",
    "    # train = get_dataset(KamitaniDataset.TRAIN)\n",
    "    # val = KamitaniDataset(FLAGS.roi, sbj_num=FLAGS.sbj_num, fmri_xfm=np.float32, subset_case=KamitaniDataset.TEST_AVG, select_voxels=FLAGS.select_voxels, is_rgbd=FLAGS.is_rgbd)\n",
    "\n",
    "    # global voxel_nc, N_VOXELS\n",
    "    # N_VOXELS = train.n_voxels\n",
    "\n",
    "\n",
    "    train, val = get_NSD_datasets(sub = FLAGS.sbj_num, roi=FLAGS.roi)\n",
    "\n",
    "    # del test    # Delete test dataset (not needed for this task)\n",
    "\n",
    "    global voxel_nc, N_VOXELS\n",
    "    N_VOXELS = train.dataset[0][1].shape[0]\n",
    "\n",
    "    trainloader = train\n",
    "    testloader = val\n",
    "\n",
    "\n",
    "\n",
    "    # voxel_nc = val.get_voxel_score('noise_ceil')\n",
    "\n",
    "    # voxel_indices_nc_sort = np.argsort(voxel_nc)[::-1]\n",
    "    # cprintm(u'(*) n_voxels: {} | noise ceiling {:.2f} \\u00B1 {:.2f} (Mean \\u00B1 SD)'.format(N_VOXELS, voxel_nc.mean(), voxel_nc.std()))\n",
    "\n",
    "    cprintm(u'(*) n_voxels: {}'.format(N_VOXELS))\n",
    "\n",
    "    if FLAGS.separable:\n",
    "        model = make_model('SeparableEncoderVGG19ml', N_VOXELS, FLAGS.random_crop_pad_percent, drop_rate=0.5)\n",
    "    else:\n",
    "        model = make_model('BaseEncoderVGG19ml', N_VOXELS, FLAGS.random_crop_pad_percent, drop_rate=0.5)\n",
    "\n",
    "    if FLAGS.is_rgbd:\n",
    "        if FLAGS.is_rgbd == 1:  # RGBD\n",
    "            if FLAGS.norm_within_img:\n",
    "                normalizer = norm_imagenet_norm_depth_img\n",
    "            else:\n",
    "                normalizer = transforms.Normalize(mean=[0.485, 0.456, 0.406, 0.461], std=[0.229, 0.224, 0.225, 0.305])  # RGBD\n",
    "        else:  # Depth only\n",
    "            normalizer = norm_depth_img\n",
    "    else:\n",
    "        normalizer = NormalizeImageNet()\n",
    "\n",
    "    img_xfm_basic = transforms.Compose([\n",
    "        transforms.Resize(size=im_res(), interpolation=Image.BILINEAR),\n",
    "        transforms.CenterCrop(im_res()),\n",
    "        transforms.ToTensor(),\n",
    "        normalizer\n",
    "        ])\n",
    "\n",
    "    img_xfm_train = transforms.Compose([\n",
    "        transforms.Resize(size=im_res(), interpolation=Image.BILINEAR),\n",
    "        transforms.RandomCrop(size=im_res(), padding=int(FLAGS.random_crop_pad_percent / 100 * im_res()), padding_mode='edge'),\n",
    "        transforms.ToTensor(),\n",
    "        normalizer\n",
    "    ])\n",
    "\n",
    "    model = model.cuda()    # Commented - achin 2704\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # # Split to train/val\n",
    "    # train = CustomDataset(train, input_xfm=img_xfm_train)\n",
    "    # val = CustomDataset(val, input_xfm=img_xfm_basic)\n",
    "\n",
    "    # # Loaders\n",
    "    # trainloader, testloader = map(lambda dset: data.DataLoader(dset, batch_size=min(FLAGS.batch_size, len(dset)), shuffle=True,\n",
    "    #                                                            num_workers=num_workers(), pin_memory=True), [train, val])\n",
    "    # Optimizer/Scheduler\n",
    "    trainable_params = chained([m.parameters() if isinstance(m, nn.Module) else [m] for m in model.trainable])\n",
    "    if FLAGS.train_bbn:\n",
    "        trainable_params0 = [x for x in trainable_params]\n",
    "        trainable_params.extend(list(model.multi_branch_bbn.parameters()))\n",
    "\n",
    "    print('    Total params for training: %s | %s' % (param_count_str(trainable_params), param_count_str(model.parameters())))\n",
    "\n",
    "    if FLAGS.train_bbn:\n",
    "        optimizer = optim.Adam([\n",
    "            {'params': trainable_params0},\n",
    "            {'params': model.multi_branch_bbn.parameters(), 'lr': 1e-6}\n",
    "            ], lr=FLAGS.learning_rate)\n",
    "    else:\n",
    "        optimizer = optim.Adam(trainable_params, lr=FLAGS.learning_rate)\n",
    "\n",
    "    model = nn.DataParallel(model)\n",
    "    if FLAGS.init_cpt_name:\n",
    "        # Load pretrained encoder\n",
    "        assert os.path.isfile(init_cpt_path())\n",
    "        print('\\t==> Loading checkpoint {}'.format(basename(init_cpt_path())))\n",
    "        model.load_state_dict(torch.load(init_cpt_path())['state_dict'])\n",
    "\n",
    "    # Regularization\n",
    "    reg_loss_dict = {}\n",
    "    tau = 800.\n",
    "    m = 1/8000.\n",
    "    def calc_mom2(W, xx, yy, dd):\n",
    "        m00 = W.view(len(W),-1).sum(1)\n",
    "        m10 = (xx * W).view(len(W), -1).sum(1)\n",
    "        m01 = (yy * W).view(len(W), -1).sum(1)\n",
    "        m02 = ((yy - (m01/m00).view(-1,1,1))**2 * W).view(len(W), -1).sum(1)\n",
    "        m20 = ((xx - (m10/m00).view(-1,1,1))**2 * W).view(len(W), -1).sum(1)\n",
    "        return (m02 + m20).sum() / (2 * dd * len(W))\n",
    "\n",
    "    xx_list = []\n",
    "    yy_list = []\n",
    "    for out_shape in model.module.out_shapes:\n",
    "        dd = out_shape[-1]\n",
    "        xx, yy = torch.tensor(np.expand_dims(np.mgrid[:dd, :dd], axis=1).repeat(N_VOXELS, axis=1), dtype=torch.float32, requires_grad=False).cuda()    # Commented - achin 2704\n",
    "        xx_list.append(xx); yy_list.append(yy)\n",
    "\n",
    "    def group_reg(reg_type='fcmom2'):\n",
    "        if FLAGS.separable:\n",
    "            loss_list = []\n",
    "            for idx, (out_shape, xx, yy) in enumerate(zip(model.module.out_shapes, xx_list, yy_list)):\n",
    "                m = model.module.space_maps[str(out_shape[-1])]\n",
    "                W_sub = list(m.parameters())[0]  # VxS\n",
    "                W_sub = W_sub.view(len(W_sub), int(np.sqrt(W_sub.shape[-1])), -1)  # VxHxW\n",
    "                if reg_type=='fcmom2':\n",
    "                    W_sub = W_sub**2\n",
    "                    loss_list.append(calc_mom2(W_sub, xx, yy, out_shape[-1]))\n",
    "                elif reg_type=='gl':\n",
    "                    Wsq_pad = F.pad(W_sub.unsqueeze(1)**2, [1, 1, 1, 1], mode='reflect').squeeze()\n",
    "                    reg_l1 = 5e-6\n",
    "                    reg_gl = 1e-5\n",
    "\n",
    "                    Wn = (Wsq_pad[..., :-2, 1:-1] + Wsq_pad[...,2:, 1:-1] + Wsq_pad[...,1:-1, :-2] + Wsq_pad[..., 1:-1, 2:])/4\n",
    "                    reg_loss = reg_l1 * W_sub.abs().sum() + reg_gl * Wn.sqrt().sum()\n",
    "                    loss_list.append(reg_loss)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            return torch.stack(loss_list).sum(0)\n",
    "        elif isinstance(model.module.fc_head, EncFCFWRF):\n",
    "            W = model.module.fc_head.rf**2\n",
    "        else:\n",
    "            W = list(model.module.fc_head[0].parameters())[0]\n",
    "            loss_list = []\n",
    "            indices_cumsum = np.insert(np.cumsum([np.prod(s) for s in model.module.out_shapes]), 0, 0)\n",
    "            for idx, (out_shape, xx, yy) in enumerate(zip(model.module.out_shapes, xx_list, yy_list)):\n",
    "                W_sub = W[:, indices_cumsum[idx]:indices_cumsum[idx+1]]\n",
    "                W_sub = W_sub.view(len(W_sub), *out_shape)  # VxCxHxW\n",
    "                if reg_type=='fcmom2':\n",
    "                    W_sub = (W_sub**2).sum(axis=1)\n",
    "                    loss_list.append(calc_mom2(W_sub, xx, yy, out_shape[-1]))\n",
    "                elif reg_type=='gl':\n",
    "                    Wsq_pad = F.pad(W_sub**2, [1, 1, 1, 1], mode='reflect')\n",
    "                    n_reg = .5\n",
    "                    ch_mult = 1.5\n",
    "                    reg_l1 = 20 * ch_mult\n",
    "                    reg_gl = 800 * ch_mult\n",
    "                    Wn = (W_sub**2 + n_reg / 4 * (Wsq_pad[..., :-2, 1:-1] + Wsq_pad[...,2:, 1:-1] + Wsq_pad[...,1:-1, :-2] + Wsq_pad[..., 1:-1, 2:])) / (1 + n_reg)\n",
    "                    reg_loss = reg_l1 * W_sub.abs().mean() + reg_gl * Wn.mean(axis=1).sqrt().mean()\n",
    "                    loss_list.append(reg_loss)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "            return torch.stack(loss_list).sum(0)\n",
    "\n",
    "    if FLAGS.separable:\n",
    "        reg_loss_dict = {\n",
    "            'L1reg_convs': (FLAGS.l1_convs, lambda : sum([param.abs().sum() for param in chained([m.parameters() for m in model.modules() if isinstance(m, nn.Conv2d)]) if param.ndim == 4])),\n",
    "            'mom2': (FLAGS.fc_mom2, lambda : group_reg('fcmom2')),\n",
    "            'gl': (FLAGS.fc_gl, lambda : group_reg('gl')),\n",
    "            'L1chan_mix': (FLAGS.l1_chan_mix, lambda : sum([chan_mix.chan_mix.abs().sum() for chan_mix in model.module.chan_mixes])),\n",
    "            'L1branch_mix': (FLAGS.l1_branch_mix, lambda : model.module.branch_mix.abs().sum()),\n",
    "    }\n",
    "    else:\n",
    "        reg_loss_dict = {\n",
    "            'L1reg_convs': (FLAGS.l1_convs, lambda : sum([param.abs().sum() for param in chained([m.parameters() for m in model.modules() if isinstance(m, nn.Conv2d)]) if param.ndim == 4])),\n",
    "            'mom2': (FLAGS.fc_mom2, lambda : group_reg('fcmom2')),\n",
    "            'gl': (FLAGS.fc_gl, lambda : group_reg('gl'))\n",
    "        }\n",
    "\n",
    "    for reg_loss_name, (w, _) in reg_loss_dict.items():\n",
    "        if callable(w) or w > 0:\n",
    "            cprintm('(+) {} {} loss.'.format(w, reg_loss_name))\n",
    "\n",
    "    scheduler = None\n",
    "    if FLAGS.scheduler > 0:\n",
    "        if FLAGS.scheduler == 1:\n",
    "            cprintm('(+) Using scheduler: On-Plateau.')\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, verbose=True, cooldown=1)\n",
    "        elif FLAGS.scheduler == 12345:  # Milestones\n",
    "            milestones = [20, 35, 45, 50]\n",
    "            cprintm('(+) Using scheduler: tenth by milestones {} epochs.'.format(', '.join([str(x) for x in milestones])))\n",
    "            scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)\n",
    "        else:\n",
    "            cprintm('(+) Using scheduler: {} every {} epochs.'.format(FLAGS.gamma, FLAGS.scheduler))\n",
    "            scheduler = optim.lr_scheduler.StepLR(optimizer, FLAGS.scheduler, gamma=FLAGS.gamma)\n",
    "\n",
    "    # Loss\n",
    "    criterion = lambda pred, actual: FLAGS.mse_loss * F.mse_loss(pred, actual) + FLAGS.cos_loss * cosine_loss(pred, actual)\n",
    "\n",
    "    # Training\n",
    "    with SummaryWriter(comment='EncTrain') if FLAGS.sum_level > 0 else dummy_context_mgr() as sum_writer:\n",
    "        global global_step\n",
    "        global_step = 0\n",
    "        best_loss = np.inf\n",
    "        with tqdm(desc='Epochs', total=FLAGS.n_epochs) if FLAGS.verbose < 1 else dummy_context_mgr() as pbar:\n",
    "            if FLAGS.verbose < 1:\n",
    "                pbar.update(0)\n",
    "            for epoch in range(FLAGS.n_epochs):\n",
    "                if FLAGS.verbose > 0:\n",
    "                    print('\\nEpoch: [%d | %d]' % (epoch + 1, FLAGS.n_epochs))\n",
    "\n",
    "                _, _, voxel_pearson_r_avg, collected_fmri = \\\n",
    "                    train_test_regress(trainloader, model, criterion, optimizer, reg_loss_dict=reg_loss_dict, sum_writer=sum_writer)\n",
    "\n",
    "\n",
    "                if FLAGS.pw_corr_win:\n",
    "                    voxel_pearson_r_pw = pearson_corr_piecewise(*collected_fmri.values(), win_size=FLAGS.pw_corr_win)\n",
    "                    sum_writer.add_figure('TrainEnc/Vox_PWCorr_vs_Corr', corr_vs_corr_plot(voxel_pearson_r_avg, voxel_pearson_r_pw, ax_labels=['Pearson R', 'PW R']), epoch)\n",
    "\n",
    "                test_loss, meters_test, voxel_pearson_r_avg, collected_test = \\\n",
    "                    train_test_regress(testloader, model, criterion, reg_loss_dict=reg_loss_dict, sum_writer=sum_writer)\n",
    "\n",
    "                # Consider test loss based on criteria\n",
    "                test_loss = dict(meters_test)['LossCriterion']\n",
    "\n",
    "                if FLAGS.pw_corr_win:\n",
    "                    voxel_pearson_r_pw = pearson_corr_piecewise(*collected_test.values(), win_size=FLAGS.pw_corr_win)\n",
    "                    sum_writer.add_figure('ValEnc/Vox_PWCorr_vs_Corr', corr_vs_corr_plot(voxel_pearson_r_avg, voxel_pearson_r_pw, ax_labels=['Pearson R', 'PW R']), epoch)\n",
    "                    collected_test = dict((k, v.flatten().tolist()) for k, v in collected_fmri.items())\n",
    "\n",
    "                if sum_writer:\n",
    "                    if epoch in [5, FLAGS.n_epochs - 1] or epoch % 10 == 0:\n",
    "                        if not FLAGS.separable:\n",
    "                            if isinstance(model.module.fc_head, EncFCFWRF):\n",
    "                                rf_params, n_out_planes = model.module.fc_head.rf, 1\n",
    "                            else:\n",
    "                                rf_params, n_out_planes = list(model.module.fc_head[0].parameters()), model.module.n_out_planes\n",
    "                            sum_writer.add_figure(\n",
    "\n",
    "                                'TrainEnc/EpochVoxRF', vox_rf(rf_params, n_out_planes, voxel_indices_nc_sort, mask=isinstance(model.module.fc_head, EncFCFWRF)), epoch)\n",
    "\n",
    "                        sum_writer.add_figure('ValEnc/OutDist', my_hist_comparison_fig(stack2numpy(collected_test), 100), epoch)\n",
    "                        # sum_writer.add_figure('ValEnc/Vox_Corr_vs_NC', corr_vs_corr_plot(voxel_nc, voxel_pearson_r_avg), epoch)\n",
    "\n",
    "\n",
    "                    for metric_name, meter_avg in meters_test:\n",
    "                        sum_writer.add_scalar('ValEnc/{}'.format(metric_name), meter_avg, epoch)\n",
    "\n",
    "                if scheduler:\n",
    "                    if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        scheduler.step(test_loss)\n",
    "                    else:\n",
    "                        scheduler.step(epoch)\n",
    "\n",
    "                # save model\n",
    "                is_best = test_loss < best_loss\n",
    "                best_loss = min(test_loss, best_loss)\n",
    "                if FLAGS.may_save:\n",
    "                    if is_best or not FLAGS.savebestonly:\n",
    "                        save_checkpoint({\n",
    "                            'epoch': epoch + 1,\n",
    "                            'state_dict': model.state_dict(),\n",
    "                            'best_loss': best_loss,\n",
    "                            'optimizer': optimizer.state_dict(),\n",
    "                        }, is_best, filepath=get_checkpoint_out())\n",
    "                if FLAGS.verbose < 1:\n",
    "                    pbar.update()\n",
    "\n",
    "        # Report\n",
    "        cprintm('    * TRAINING COMPLETE *')\n",
    "    cprint1(FLAGS.exp_prefix)\n",
    "    with open(f'{PROJECT_ROOT}/runs/{FLAGS.exp_prefix}.txt', 'w') as f:\n",
    "        f.write(FLAGS.flags_into_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bdb48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_regress(loader, model, criterion, optimizer=None, reg_loss_dict={},  sum_writer=None):\n",
    "    global global_step\n",
    "    if optimizer:\n",
    "        mode = 'Train'\n",
    "        # switch to train mode\n",
    "        model.train()\n",
    "    else:\n",
    "        mode = 'Val'\n",
    "        model.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses_names = ['total'] + ['criterion', 'mae', 'cosine_loss'] + list(reg_loss_dict.keys())\n",
    "    losses = dict(zip(losses_names, [AverageMeter() for _ in range(len(losses_names))]))\n",
    "    corrs = { metric_tup: AverageMeter() for metric_tup in [(np.median, 'R_median'),\n",
    "                                                            (lambda x: np.percentile(x, 90), 'R_90'),\n",
    "                                                            (lambda x: np.percentile(x, 75), 'R_75'),\n",
    "                                                            ]}\n",
    "    corrs_nc_norm = copy.deepcopy(corrs)\n",
    "    corrs_nc_norm = { (metric_func, metric_name.replace('R', 'R_ncnorm')): meter for (metric_func, metric_name), meter in corrs_nc_norm.items() }\n",
    "    voxel_pearson_r = AverageMeter()\n",
    "    end = time.time()\n",
    "    collected_fmri = {'actual': [], 'pred': []}\n",
    "    with tqdm(desc=mode, total=len(loader)) if FLAGS.verbose > 0 else dummy_context_mgr() as bar:\n",
    "        for batch_idx, (images, fmri_actual) in enumerate(loader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if FLAGS.sum_level > 3:\n",
    "                if FLAGS.sum_level > 4 or not optimizer or FLAGS.pw_corr_win:\n",
    "                    actual_fmri_list = list(np.squeeze(sample_portion(fmri_actual, 1/10).flatten()))\n",
    "                    if FLAGS.pw_corr_win:\n",
    "                        collected_fmri['actual'].append(fmri_actual)\n",
    "                    else:\n",
    "                        collected_fmri['actual'].extend(actual_fmri_list)\n",
    "\n",
    "            fmri_actual = fmri_actual.cuda()    # Commented - achin 2704\n",
    "            images = images.cuda()    # Commented - achin 2704\n",
    "\n",
    "            # import pdb; pdb.set_trace()\n",
    "\n",
    "            with dummy_context_mgr() if optimizer else torch.no_grad():\n",
    "                fmri_pred = model(images, detach_bbn=FLAGS.allow_bbn_detach)\n",
    "\n",
    "            loss_criterion = criterion(fmri_pred, fmri_actual)\n",
    "            losses['criterion'].update(loss_criterion.data, fmri_actual.size(0))\n",
    "            losses['mae'].update(F.l1_loss(fmri_pred, fmri_actual).data, fmri_actual.size(0))\n",
    "            losses['cosine_loss'].update(cosine_loss(fmri_pred, fmri_actual).data, fmri_actual.size(0))\n",
    "\n",
    "            reg_loss_tot = 0\n",
    "            for loss_name, (w, reg_loss_func) in reg_loss_dict.items():\n",
    "                reg_loss = reg_loss_func()\n",
    "                losses[loss_name].update(reg_loss.data, fmri_actual.size(0))\n",
    "                if callable(w):\n",
    "                    w = w(global_step)\n",
    "                reg_loss_tot += w * reg_loss\n",
    "\n",
    "            loss = loss_criterion + reg_loss_tot\n",
    "\n",
    "            losses['total'].update(loss.data, fmri_actual.size(0))\n",
    "\n",
    "            voxel_pearson_r.update(pearson_corr(fmri_pred.data, fmri_actual.data).cpu().numpy())\n",
    "            for (metric_func, _), meter in corrs.items():\n",
    "                meter.update(metric_func(voxel_pearson_r.val), fmri_actual.size(0))\n",
    "\n",
    "            # voxel_pearson_r_ncnorm = voxel_pearson_r.val / voxel_nc\n",
    "\n",
    "            # for (metric_func, _), meter in corrs_nc_norm.items():\n",
    "            #     meter.update(metric_func(voxel_pearson_r_ncnorm), fmri_actual.size(0))\n",
    "\n",
    "            if FLAGS.sum_level > 3:\n",
    "                if FLAGS.sum_level > 4 or not optimizer or FLAGS.pw_corr_win:\n",
    "                    pred_fmri_list = list(np.squeeze(sample_portion(fmri_pred, 1 / 10).cpu().detach().flatten()))\n",
    "                    if FLAGS.pw_corr_win:\n",
    "                        collected_fmri['pred'].append(fmri_pred.cpu().detach())\n",
    "                    else:\n",
    "                        collected_fmri['pred'].extend(pred_fmri_list)\n",
    "\n",
    "            if optimizer:\n",
    "                model.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                global_step += 1\n",
    "                if sum_writer:\n",
    "                    for loss_name, meter in losses.items():\n",
    "                        sum_writer.add_scalar('TrainEnc/Loss{}'.format(loss_name.capitalize()), meter.val, global_step)\n",
    "                    for (_, metric_name), meter in corrs.items():\n",
    "                        sum_writer.add_scalar('TrainEnc/{}'.format(metric_name), meter.val, global_step)\n",
    "                    for (_, metric_name), meter in corrs_nc_norm.items():\n",
    "                        sum_writer.add_scalar('TrainEnc/{}'.format(metric_name), meter.val, global_step)\n",
    "\n",
    "                    if FLAGS.sum_level > 4:\n",
    "                        if (global_step - 1) % 5 == 0:\n",
    "                            fig = hist_comparison_fig(stack2numpy({'actual': actual_fmri_list, 'pred': pred_fmri_list}), linspace(-2.5, 2.5, 100))\n",
    "                            sum_writer.add_figure('TrainEnc/BatchDist', fig, global_step)\n",
    "\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if FLAGS.verbose > 0:  # plot progress\n",
    "                bar.set_postfix_str(\n",
    "                    '({batch}/{size}) Data: {data:.3f}s | Batch: {bt:.3f}s | LossAvg: {lossavg:.4f} ({loss:.4f})'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(loader),\n",
    "                        data=data_time.avg,\n",
    "                        bt=batch_time.avg,\n",
    "                        lossavg=losses['total'].avg,\n",
    "                        loss=losses['total'].val,\n",
    "                    ))\n",
    "                bar.update()\n",
    "\n",
    "    if FLAGS.pw_corr_win:\n",
    "        collected_fmri = dict((k, torch.cat(v)) for k, v in collected_fmri.items())\n",
    "    return losses['total'].avg, \\\n",
    "           [('Loss' + loss_name.capitalize(), meter.avg) for loss_name, meter in losses.items()] + \\\n",
    "           [(metric_name, meter.avg) for (_, metric_name), meter in corrs.items()] + \\\n",
    "           [(metric_name, meter.avg) for (_, metric_name), meter in corrs_nc_norm.items()], \\\n",
    "           voxel_pearson_r.avg, collected_fmri\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dc5f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
